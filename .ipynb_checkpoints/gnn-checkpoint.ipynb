{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "my_batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "\n",
    "class MyDataset(DGLDataset):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -------------------------\n",
    "    raw_dir: str\n",
    "        Specifying the directory that already stores the input data.\n",
    "    \n",
    "    \"\"\"\n",
    "    _pos_directory= '../positive_graph_save/'\n",
    "    _neg_directory= '../negative_graph_save/'\n",
    "    def __init__(self, \n",
    "                 url=None,\n",
    "                 raw_dir=None,\n",
    "                 save_dir=None,\n",
    "                 force_reload=False,\n",
    "                 verbose=False):\n",
    "        super(MyDataset, self).__init__(name='docking_classify',\n",
    "                                        url=url,\n",
    "                                        raw_dir=raw_dir,\n",
    "                                        save_dir=save_dir,\n",
    "                                        force_reload=force_reload,\n",
    "                                        verbose=verbose)\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    #must be implemented\n",
    "    def process(self):\n",
    "        df_pos = pd.read_csv('./positive_dataset.csv')\n",
    "        df_neg = pd.read_csv('./negative_dataset.csv')\n",
    "        pos_graphs = df_pos['file_name']\n",
    "        pos_labels = df_pos['label']\n",
    "        neg_graphs = df_neg['file_name']\n",
    "        neg_labels = df_neg['label']\n",
    "\n",
    "        #half_batch = int(my_batch_size/2)\n",
    "        self.graph_dataset = []\n",
    "        self.graph_labels = []\n",
    "        #negative graphs are more\n",
    "        for i in range(len(neg_graphs)):\n",
    "            self.graph_dataset.append(pos_graphs[i%len(pos_graphs)])\n",
    "            self.graph_dataset.append(neg_graphs[i])\n",
    "            self.graph_labels.append(torch.Tensor([1,0])) #positive\n",
    "            self.graph_labels.append(torch.Tensor([0,1])) #negative\n",
    "            \n",
    "        self.df_dataset = pd.DataFrame({'file_name':self.graph_dataset, 'label':self.graph_labels})\n",
    "        self.df_dataset = shuffle(self.df_dataset)\n",
    "        #for i in range(len())\n",
    "\n",
    "    \n",
    "    #must be implemented\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"get one item by index\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        idx: int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        ---------------\n",
    "        (dgl.DGLGraph, Tensor)\n",
    "        \"\"\"\n",
    "        graph = dgl.load_graphs(self.df_dataset['file_name'][idx.item()])[0] #idx.item():convert torch.Tensor to int\n",
    "        #print(self.df_dataset['file_name'][idx.item()])\n",
    "        label = self.df_dataset['label'][idx.item()]\n",
    "        return graph[0], label[0].long()\n",
    "\n",
    "    #must be implemented\n",
    "    def __len__(self):\n",
    "        #number of data examples\n",
    "        return self.df_dataset.shape[0]\n",
    "        \n",
    "\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def has_cache(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 14996\n"
     ]
    }
   ],
   "source": [
    "from dgl.dataloading.pytorch import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(my_dataset)\n",
    "print(\"dataset length:\", num_examples)\n",
    "num_train = int(num_examples*0.8)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
    "\n",
    "train_dataloader = GraphDataLoader(my_dataset, sampler=train_sampler, batch_size=my_batch_size, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(my_dataset, sampler=test_sampler, batch_size=my_batch_size, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use $train\\_dataloader$ and $test\\_dataloader$ to get a batched graph with a batch size of $my\\_batch\\_size$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=68123, num_edges=64322414,\n",
      "      ndata_schemes={'h': Scheme(shape=(10,), dtype=torch.int64)}\n",
      "      edata_schemes={'h': Scheme(shape=(1,), dtype=torch.float32)}), tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size, allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')\n",
    "gnn = GCN(10, 16, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(gnn.parameters(), lr=0.01)\n",
    "all_logits = []\n",
    "for epoch in range(30):\n",
    "    for batched_graph, labels in tqdm(train_dataloader):\n",
    "        pred = gnn(batched_graph, batched_graph.ndata['h'].float())\n",
    "        #print(pred.shape)\n",
    "        #print(labels.shape)\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"batches:\"+str(batch)+\"------------------------loss:\"+str(loss))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
