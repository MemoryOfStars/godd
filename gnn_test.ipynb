{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/600 [00:00<00:11, 52.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 14996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 442/600 [00:07<00:02, 56.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-40c9009c6a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDGLDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgnn_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/bioinfo/python_files/gnn_train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from torch.autograd import Variable\n",
    "from dgl.data import DGLDataset\n",
    "from sklearn.utils import shuffle\n",
    "from gnn_train.pyimport GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 4648\n"
     ]
    }
   ],
   "source": [
    "my_batch_size = 30\n",
    "\n",
    "class MyDataset(DGLDataset):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -------------------------\n",
    "    raw_dir: str\n",
    "        Specifying the directory that already stores the input data.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 url=None,\n",
    "                 raw_dir=None,\n",
    "                 save_dir=None,\n",
    "                 force_reload=False,\n",
    "                 verbose=False):\n",
    "        super(MyDataset, self).__init__(name='docking_classify',\n",
    "                                        url=url,\n",
    "                                        raw_dir=raw_dir,\n",
    "                                        save_dir=save_dir,\n",
    "                                        force_reload=force_reload,\n",
    "                                        verbose=verbose)\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    #must be implemented\n",
    "    def process(self):\n",
    "        df_pos = pd.read_csv('./positive_test_dataset.csv')\n",
    "        df_neg = pd.read_csv('./negative_test_dataset.csv')\n",
    "        pos_graphs = df_pos['file_name']\n",
    "        pos_labels = df_pos['label']\n",
    "        neg_graphs = df_neg['file_name']\n",
    "        neg_labels = df_neg['label']\n",
    "\n",
    "        #half_batch = int(my_batch_size/2)\n",
    "        self.graph_dataset = []\n",
    "        self.graph_labels = []\n",
    "        #negative graphs are more\n",
    "        for i in range(len(neg_graphs)):\n",
    "            self.graph_dataset.append(pos_graphs[i%len(pos_graphs)])\n",
    "            self.graph_dataset.append(neg_graphs[i])\n",
    "            self.graph_labels.append(torch.Tensor([1])) #positive\n",
    "            self.graph_labels.append(torch.Tensor([0])) #negative\n",
    "            \n",
    "        self.df_dataset = pd.DataFrame({'file_name':self.graph_dataset, 'label':self.graph_labels})\n",
    "        self.df_dataset = shuffle(self.df_dataset)\n",
    "        #for i in range(len())\n",
    "\n",
    "    \n",
    "    #must be implemented\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"get one item by index\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        idx: int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        ---------------\n",
    "        (dgl.DGLGraph, Tensor)\n",
    "        \"\"\"\n",
    "        graph = dgl.load_graphs(self.df_dataset['file_name'][idx.item()])[0] #idx.item():convert torch.Tensor to int\n",
    "        #print(self.df_dataset['file_name'][idx.item()])\n",
    "        label = self.df_dataset['label'][idx.item()]\n",
    "        return graph[0], label[0].float()\n",
    "\n",
    "    #must be implemented\n",
    "    def __len__(self):\n",
    "        #number of data examples\n",
    "        return self.df_dataset.shape[0]\n",
    "        \n",
    "\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def has_cache(self):\n",
    "        pass\n",
    "\n",
    "my_dataset = MyDataset()\n",
    "\n",
    "from dgl.dataloading.pytorch import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(my_dataset)\n",
    "print(\"dataset length:\", num_examples)\n",
    "\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_examples))\n",
    "test_dataloader = GraphDataLoader(my_dataset, sampler=test_sampler, batch_size=my_batch_size, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, 80, allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(80, 160, allow_zero_in_degree=True)\n",
    "        self.conv3 = GraphConv(160, 112, allow_zero_in_degree=True)\n",
    "        self.conv4 = GraphConv(112, 160, allow_zero_in_degree=True)\n",
    "        self.conv5 = GraphConv(160, 176, allow_zero_in_degree=True)\n",
    "        self.conv6 = GraphConv(176, 96, allow_zero_in_degree=True)\n",
    "        self.conv7 = GraphConv(96, 144, allow_zero_in_degree=True)\n",
    "        self.conv8 = GraphConv(144, 96, allow_zero_in_degree=True)\n",
    "        self.conv9 = GraphConv(96, 128, allow_zero_in_degree=True)\n",
    "        self.conv10 = GraphConv(128, 96, allow_zero_in_degree=True)\n",
    "        self.conv11 = GraphConv(96, 160, allow_zero_in_degree=True)\n",
    "        self.dnn1 = torch.nn.Linear(160, 140)\n",
    "        self.dnn2  = torch.nn.Linear(140, num_classes)\n",
    "        param_mu = torch.tensor(0.0)\n",
    "        param_sigma = torch.tensor(1.0)\n",
    "        self.param_mu = nn.Parameter(param_mu)\n",
    "        self.param_sigma = nn.Parameter(param_sigma)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        pow_param = torch.mul(g.edata['h'] - self.param_mu, g.edata['h'] - self.param_mu)/(-self.param_sigma)\n",
    "        efeat = torch.log(pow_param)\n",
    "        g.edata['h'] = efeat\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv3(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv4(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv5(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv6(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv7(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv8(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv9(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv10(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.conv11(g, h)\n",
    "        h = F.leaky_relu(h)\n",
    "        g.ndata['h'] = h\n",
    "        h = dgl.mean_nodes(g, 'h')\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dnn1(h)\n",
    "        h = F.dropout(h, p=0.3)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dnn2(h)\n",
    "        h = F.dropout(h, p=0.2)\n",
    "        h = torch.sigmoid(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GraphConv(in=44, out=80, normalization=both, activation=None)\n",
       "  (conv2): GraphConv(in=80, out=160, normalization=both, activation=None)\n",
       "  (conv3): GraphConv(in=160, out=112, normalization=both, activation=None)\n",
       "  (conv4): GraphConv(in=112, out=160, normalization=both, activation=None)\n",
       "  (conv5): GraphConv(in=160, out=176, normalization=both, activation=None)\n",
       "  (conv6): GraphConv(in=176, out=96, normalization=both, activation=None)\n",
       "  (conv7): GraphConv(in=96, out=144, normalization=both, activation=None)\n",
       "  (conv8): GraphConv(in=144, out=96, normalization=both, activation=None)\n",
       "  (conv9): GraphConv(in=96, out=128, normalization=both, activation=None)\n",
       "  (conv10): GraphConv(in=128, out=96, normalization=both, activation=None)\n",
       "  (conv11): GraphConv(in=96, out=160, normalization=both, activation=None)\n",
       "  (dnn1): Linear(in_features=160, out_features=140, bias=True)\n",
       "  (dnn2): Linear(in_features=140, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelPath = '../models/gcn1636930820.pkl'\n",
    "model = torch.load(modelPath)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6034853700516352"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_tests = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "device = torch.device(\"cuda:0\")\n",
    "for batched_graph, labels in test_dataloader:\n",
    "    batched_graph, labels = batched_graph.to(device), labels.to(device)\n",
    "    pred = model(batched_graph, batched_graph.ndata['h'].float()).squeeze(1).squeeze(1)\n",
    "    # print(pred, labels)\n",
    "    for i, p in enumerate(pred.round()):\n",
    "        if p != labels[i]:\n",
    "            FP += 1 if p == torch.tensor(1.0) else 0\n",
    "            FN += 1 if p == torch.tensor(0.0) else 0\n",
    "\n",
    "    num_correct += (pred.round() == labels).sum().item() # TP+TN\n",
    "    num_tests += len(labels) # TP+TN+FP+FN\n",
    "num_correct/num_tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
